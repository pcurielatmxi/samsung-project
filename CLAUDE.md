# Samsung Taylor FAB1 Performance Analysis

**Project:** Data-driven analysis of schedule delays and labor consumption
**Client:** Samsung
**Analyst:** MXI

---

## Project Phases

### Phase 1: Data Collection & Initial Assessment âœ…

**Status:** Complete

Established data pipelines for 9 primary sources:

| Source | Purpose | Records | Status |
|--------|---------|---------|--------|
| Primavera P6 | Schedule snapshots (YATES + SECAI) | 66 files, 964K tasks | âœ… Processed |
| Weekly Reports | Issues, progress, manpower | 37 reports, 1,108 issues | âœ… Processed |
| TBM Daily Plans | Daily work activities by crew | 13.5K entries | âœ… Processed |
| ProjectSight | Daily reports, labor hours | 857K labor entries | âœ… Processed |
| ProjectSight NCR | Quality non-conformance records (Notice to Comply) | 1,985 records (NCR/QOR/SOR/SWN/VR) | âœ… Processed |
| Quality Records | Inspections (Yates WIR + SECAI IR) | 37K inspections | âœ… Processed |
| RABA | Quality inspections (RKCI Celvis) | 995+ daily batches | âœ… Scraped |
| PSI | Quality inspections (Construction Hive) | 6,309 reports | âœ… Scraped |
| QC Logs | Inspection request tracking (CPMS exports) | 61K+ records, 141 files | ðŸ“ Raw |
| Fieldwire | Punch lists, field tasks | TBD | ðŸ”„ In Progress |
| Narratives | P6 narratives, weekly report narratives, milestone variance | ~80 documents | ðŸ”„ Processing |

**Key Deliverables:**
- Parsed CSV tables for all sources in `data/processed/`
- WBS taxonomy classifier (`src/classifiers/task_classifier.py`)
- Quality taxonomy extraction (`scripts/quality/derive/`)
- Data source documentation (`docs/SOURCES.md`, `docs/DATA_SOURCE_NOTES.md`)

### Phase 2: Integrated Analysis ðŸ”„

**Status:** In Progress

**Goal:** Link all data sources through a unified location model to enable cross-dataset analysis.

**Primary Objective:** Answer "What quality issues occurred WHERE, by WHOM, and how much rework did they cause?"

#### The Integration Challenge

Each data source has different location granularity:

| Source | Location Data | Linkage Key |
|--------|---------------|-------------|
| P6 Tasks | Room codes (FAB112345), Building, Level | `location_code` |
| RABA/PSI | Building, Level, Grid (e.g., G/10) | Grid coordinates |
| Labor Hours | Company only | Company â†’ Trade â†’ Location inference |

#### Solution: `dim_location` with Grid Bounds

The centerpiece is a location dimension table where every room/elevator/stair has **grid bounds** (row_min/max, col_min/max). This enables:

1. **Room â†’ Grid**: Look up grid bounds for any room code
2. **Grid â†’ Room(s)**: Reverse lookup - find which rooms contain a grid coordinate
3. **Company â†’ Location**: Infer from quality inspection patterns (e.g., "Berg works drywall on SUE levels 2-4")

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       dim_location                               â”‚
â”‚  location_code â”‚ building â”‚ level â”‚ grid_row_min/max â”‚ grid_col_min/max â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FAB112345     â”‚ SUE      â”‚ 1F    â”‚ B / E            â”‚ 5 / 12           â”‚
â”‚  ELV-S         â”‚ SUW      â”‚ 2F    â”‚ L / M            â”‚ 17 / 17          â”‚
â”‚  ...           â”‚          â”‚       â”‚                  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    P6 Tasks     â”‚           â”‚  Quality Data   â”‚
â”‚ JOIN ON         â”‚           â”‚ SPATIAL JOIN    â”‚
â”‚ location_code   â”‚           â”‚ WHERE grid IN   â”‚
â”‚                 â”‚           â”‚ (row/col bounds)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Location Master Status

| Location Type | Total | With Grid Bounds | Status |
|---------------|-------|------------------|--------|
| ROOM | 360 | ~60 | Needs manual lookup from drawings |
| ELEVATOR | 13 | 13 | Complete |
| STAIR | 25 | ~10 | Partial |
| GRIDLINE | 35 | 35 | Auto-generated (full row span) |
| LEVEL/AREA | 90 | N/A | Special cases |

**Working File:** `raw/location_mappings/location_master.csv`
**Grid Source:** `raw/location_mappings/Samsung_FAB_Codes_by_Gridline_3.xlsx`

#### Key Scripts

| Script | Purpose |
|--------|---------|
| `scripts/primavera/derive/generate_location_master.py` | Generate location master from P6 taxonomy |
| `scripts/shared/gridline_mapping.py` | Low-level grid coordinate lookup |
| `scripts/shared/location_model.py` | High-level location API (forward/reverse lookups) |
| `scripts/shared/company_standardization.py` | Company/trade/category normalization |

#### Deliverables

- `dim_location` - All locations with grid bounds (in progress)
- `dim_company` - Master company list with alias resolution
- `dim_trade` - Trade/work type classification
- `map_company_location` - Company work areas by period (derived from quality data)

### Phase 3: Analysis & Conclusions (Planned)

**Status:** Not Started

Four analysis tracks per Executive Summary:
1. Scope Evolution - Task growth attribution
2. Delay Attribution - Critical path impact analysis
3. Resource Consumption - Labor hours correlation
4. Quality Impact - Rework quantification

---

## Repository Structure

```
samsung-project/
â”œâ”€â”€ CLAUDE.md                    # This file - project overview
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ EXECUTIVE_SUMMARY.md     # Analysis goals and status
â”‚   â”œâ”€â”€ SOURCES.md               # Data source inventory
â”‚   â”œâ”€â”€ DATA_SOURCE_NOTES.md     # Technical parsing notes
â”‚   â””â”€â”€ analysis/                # Analysis documentation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ shared/                  # Cross-source utilities (location model, standardization)
â”‚   â”œâ”€â”€ primavera/               # P6 XER parsing and analysis
â”‚   â”œâ”€â”€ weekly_reports/          # PDF report parsing
â”‚   â”œâ”€â”€ tbm/                     # TBM Excel parsing
â”‚   â”œâ”€â”€ projectsight/            # ProjectSight export processing
â”‚   â”œâ”€â”€ quality/                 # Quality record processing
â”‚   â”œâ”€â”€ raba/                    # RABA scraper + document_processing config
â”‚   â”œâ”€â”€ psi/                     # PSI scraper + document_processing config
â”‚   â”œâ”€â”€ narratives/              # Narratives document_processing config
â”‚   â””â”€â”€ integrated_analysis/     # Phase 2 - cross-source integration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/settings.py       # Path configuration
â”‚   â”œâ”€â”€ classifiers/             # WBS taxonomy classifier
â”‚   â””â”€â”€ document_processor/      # Centralized N-stage document processing
â””â”€â”€ data/                        # Git-tracked analysis outputs
```

## Data Directory Structure

External data (not in repo) follows traceability classification:

```
{WINDOWS_DATA_DIR}/
â”œâ”€â”€ raw/{source}/           # Source files exactly as received
â”‚                           # 100% traceable to external source
â”œâ”€â”€ processed/{source}/     # Parsed/transformed data
â”‚                           # 100% traceable to raw/
â””â”€â”€ derived/{source}/       # Enhanced data with inference
                            # Includes assumptions - NOT fully traceable
```

## Key Configuration

- **Settings:** `src/config/settings.py` - All path constants
- **Environment:** `.env` file with `WINDOWS_DATA_DIR` path
- **Python Environment:** Use the existing `.venv` virtual environment for all Python scripts and installations. Activate with `source .venv/bin/activate`

## Analysis Objectives

1. **Scope Evolution** - How much scope was added due to rework or coordination issues?
2. **Delay Attribution** - How much schedule impact resulted from quality and performance issues?
3. **Resource Consumption** - How much labor was consumed and what factors drove consumption?
4. **Quality Impact** - What quality issues occurred and what rework did they drive?

## Data Traceability

All analysis must maintain traceability to source documents:
- `raw/` data is untouched source files
- `processed/` data is direct transformation (fully traceable)
- `derived/` data includes assumptions (document methodology)

See [.claude/skills/mxi-powerpoint/SKILL.md](.claude/skills/mxi-powerpoint/SKILL.md) for presentation data traceability requirements.

## Quality Data Architecture

Quality inspection data is central to the project's rework and delay analysis. Three complementary data sources provide different views of the same inspection events:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUALITY DATA SOURCES                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  QC Logs (CPMS)              RABA (RKCI Celvis)    PSI (Const Hive) â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•            â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚  Master tracking list        3rd-party QC firm    3rd-party QC firmâ”‚
â”‚  61K+ inspection requests    9K+ inspection recs  6K+ field reportsâ”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ IR Number     â”‚         â”‚ â€¢ Full PDF   â”‚     â”‚ â€¢ Full PDF   â”‚ â”‚
â”‚  â”‚ â€¢ Date/Time     â”‚  â”€â”€â”€â–º   â”‚ â€¢ Photos     â”‚     â”‚ â€¢ Photos     â”‚ â”‚
â”‚  â”‚ â€¢ Status        â”‚  Detail â”‚ â€¢ Signatures â”‚     â”‚ â€¢ Checklists â”‚ â”‚
â”‚  â”‚ â€¢ Template      â”‚  View   â”‚ â€¢ Findings   â”‚     â”‚ â€¢ Findings   â”‚ â”‚
â”‚  â”‚ â€¢ Location      â”‚         â”‚ â€¢ Defects    â”‚     â”‚ â€¢ Defects    â”‚ â”‚
â”‚  â”‚ â€¢ Failure reasonâ”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚        List View                    Detail Views                    â”‚
â”‚                                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RELATIONSHIP: QC Logs tracks ALL inspection requests from ALL     â”‚
â”‚  contractors. RABA and PSI contain the actual inspection reports   â”‚
â”‚  with photos, signatures, and detailed findings. Some QC Log       â”‚
â”‚  entries reference PSI via "Duplicate of ProjectSight#XXXX".       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CONTRACTOR COVERAGE (from QC Logs IR Number prefixes):            â”‚
â”‚  â€¢ YT (Yates): 15K records        â€¢ SECAI: 2K records              â”‚
â”‚  â€¢ AG/ABR (Austin Bridge): 4K     â€¢ 50+ subcontractors: 40K        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  USE CASES:                                                         â”‚
â”‚  â€¢ QC Logs â†’ Volume trends, pass/fail rates, contractor metrics    â”‚
â”‚  â€¢ RABA/PSI â†’ Defect details, root cause analysis, photo evidence  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:** QC Logs provides the "what happened" summary (pass/fail, counts, dates), while RABA/PSI provide the "why it happened" evidence (detailed PDF reports with photos and findings).

---

## Tools

### Document Processor (Centralized N-Stage Pipeline)

**Location:** [src/document_processor/](src/document_processor/)

Centralized document processing pipeline with flexible N-stage support, implicit stage chaining, and LLM-based quality checking.

**Architecture:**
```
src/document_processor/
â”œâ”€â”€ pipeline.py           # N-stage runner with QC integration
â”œâ”€â”€ config.py             # N-stage config loader
â”œâ”€â”€ cli.py                # Unified CLI entry point
â”œâ”€â”€ quality_check.py      # QC sampling and halt logic
â”œâ”€â”€ stages/               # Stage implementations
â”‚   â”œâ”€â”€ llm_stage.py      # Gemini PDF/text processing
â”‚   â””â”€â”€ script_stage.py   # Python postprocessing
â”œâ”€â”€ clients/
â”‚   â””â”€â”€ gemini_client.py  # Gemini API wrapper
â””â”€â”€ utils/
    â”œâ”€â”€ file_utils.py     # Atomic writes, error files
    â””â”€â”€ status.py         # Pipeline status analysis
```

**Stage Types:**
- `llm`: Process documents with Gemini (PDF native upload, DOCX/XLSX text extraction)
- `script`: Custom Python postprocessing (location parsing, date normalization, etc.)

**Features:**
- Flexible N stages (not fixed to 2) - configure as many as needed
- Implicit chaining: Stage 1 uses input files, Stage N uses Stage N-1 output
- Per-stage QC prompts with automatic halt on >10% failure rate
- Idempotency via `.error.json` files
- Sequential numbered output folders (1.extract/, 2.format/, 3.clean/)
- Source-specific configs in `scripts/{source}/document_processing/`

**Config Schema (config.json):**
```json
{
  "input_dir": "${WINDOWS_DATA_DIR}/raw/raba/individual",
  "output_dir": "${WINDOWS_DATA_DIR}/processed/raba",
  "file_extensions": [".pdf"],
  "concurrency": 5,
  "qc_batch_size": 50,
  "qc_failure_threshold": 0.10,
  "stages": [
    {"name": "extract", "type": "llm", "model": "gemini-3-flash-preview", "prompt_file": "extract_prompt.txt"},
    {"name": "format", "type": "llm", "model": "gemini-3-flash-preview", "prompt_file": "format_prompt.txt", "schema_file": "schema.json"},
    {"name": "clean", "type": "script", "script": "postprocess.py", "function": "process_record"}
  ]
}
```

**Usage:**
```bash
# Run all stages
python -m src.document_processor scripts/raba/document_processing/

# Run specific stage
python -m src.document_processor scripts/raba/document_processing/ --stage extract

# Show status
python -m src.document_processor scripts/raba/document_processing/ --status

# Common options
--force              # Reprocess completed files
--retry-errors       # Retry failed files only
--limit N            # Process N files max
--dry-run            # Preview without processing
--bypass-qc-halt     # Continue despite QC halt file
--disable-qc         # Skip quality checks
```

**Quality Check System:**
- Samples 1 file per `qc_batch_size` files processed
- QC prompt verifies inputâ†’output quality with LLM
- Tracks failure rate; halts if >10% after min samples
- Creates `.qc_halt.json` requiring prompt fix or `--bypass-qc-halt`

**Output Structure:**
```
processed/raba/
â”œâ”€â”€ 1.extract/
â”‚   â”œâ”€â”€ {file}.extract.json
â”‚   â””â”€â”€ {file}.extract.error.json
â”œâ”€â”€ 2.format/
â”‚   â””â”€â”€ {file}.format.json
â”œâ”€â”€ 3.clean/
â”‚   â””â”€â”€ {file}.clean.json
â””â”€â”€ .qc_halt.json  # If QC failure rate exceeded
```

### Source-Specific Pipelines

Each data source has its own config in `scripts/{source}/document_processing/`:

| Source | Stages | Config Location |
|--------|--------|-----------------|
| RABA | extract â†’ format â†’ clean | `scripts/raba/document_processing/` |
| PSI | extract â†’ format â†’ clean | `scripts/psi/document_processing/` |
| Narratives | extract (only) | `scripts/narratives/document_processing/` |

**RABA/PSI Pipeline:**
```bash
cd scripts/raba/document_processing
./run.sh status              # Check progress
./run.sh test 10             # Dry run 10 files
./run.sh extract --limit 50  # Extract 50 files
./run.sh format              # Format all extracted
./run.sh clean               # Normalize all formatted
./run.sh retry               # Retry failures
```

**Narratives Pipeline (single stage):**
```bash
cd scripts/narratives/document_processing
./run.sh extract --limit 10  # Extract narrative documents
./run.sh status              # Check progress
```

### RABA Quality Reports Scraper

**Location:** [scripts/raba/process/scrape_raba_reports.py](scripts/raba/process/scrape_raba_reports.py)

A Playwright-based automation tool for downloading quality inspection reports from the RABA (RKCI Celvis) system. Downloads daily batch PDFs containing all inspection reports for each date.

**Features:**
- Automated login with credentials from `.env`
- Daily batch downloads - selects all reports for a date and downloads as single PDF
- Idempotent operation via `manifest.json` tracking
- Empty date tracking (dates with no reports marked in manifest to avoid retries)
- Resume capability - skips already downloaded dates
- `--force` flag to re-download specific dates

**Output Structure:**
```
{WINDOWS_DATA_DIR}/raw/raba/
â”œâ”€â”€ daily/
â”‚   â”œâ”€â”€ 2023-06-01.pdf    # All reports for June 1, 2023
â”‚   â”œâ”€â”€ 2023-06-02.pdf    # All reports for June 2, 2023
â”‚   â””â”€â”€ ...
â””â”€â”€ manifest.json          # Download tracking
```

**Usage:**
```bash
# Download date range
python scripts/raba/process/scrape_raba_reports.py --start 2023-06-01 --end 2023-06-30

# Force re-download
python scripts/raba/process/scrape_raba_reports.py --start 2023-06-01 --end 2023-06-01 --force

# Non-headless mode for debugging
python scripts/raba/process/scrape_raba_reports.py --start 2023-06-01 --end 2023-06-01 --no-headless
```

**Environment Variables (`.env`):**
- `RABA_BASE_URL` - Login URL
- `RABA_USERNAME` - Login username
- `RABA_PASSWORD` - Login password

### RABA Individual Reports Scraper (Recommended)

**Location:** [scripts/raba/process/scrape_raba_individual.py](scripts/raba/process/scrape_raba_individual.py)

An improved Playwright-based automation tool that downloads RABA reports as individual PDFs (one per inspection assignment). This approach is preferred over the batch scraper because:
- No need to split multi-report batch PDFs afterward
- Works for days with only 1 report (batch button unavailable in RABA UI)
- Each PDF is named by assignment number for direct processing
- Handles pagination for months with many reports

**Features:**
- Automated login with credentials from `.env`
- Month-by-month processing with pagination support
- Individual PDF downloads named by assignment number (e.g., `A22-016871.pdf`)
- Idempotent operation via `manifest.json` tracking each report
- Resume capability - skips already downloaded reports
- `--force` flag to re-download existing files
- `--limit` flag for testing (limits total downloads)

**Output Structure:**
```
{WINDOWS_DATA_DIR}/raw/raba/
â”œâ”€â”€ individual/
â”‚   â”œâ”€â”€ A22-016104.pdf    # Individual inspection report
â”‚   â”œâ”€â”€ A22-016105.pdf
â”‚   â”œâ”€â”€ A22-016871.pdf
â”‚   â””â”€â”€ ...
â””â”€â”€ individual_manifest.json   # Download tracking with metadata per report
```

**Usage:**
```bash
# Download all reports from project start (May 2022) to now
python scripts/raba/process/scrape_raba_individual.py

# Download specific date range
python scripts/raba/process/scrape_raba_individual.py --start-date 2022-06-01 --end-date 2022-06-30

# Test with limit
python scripts/raba/process/scrape_raba_individual.py --limit 10

# Force re-download
python scripts/raba/process/scrape_raba_individual.py --start-date 2022-06-01 --end-date 2022-06-30 --force

# Run headless (for background operation)
python scripts/raba/process/scrape_raba_individual.py --headless
```

**Environment Variables (`.env`):**
- `RABA_USERNAME` - Login username
- `RABA_PASSWORD` - Login password

### PSI Quality Reports Scraper

**Location:** [scripts/psi/process/scrape_psi_reports.py](scripts/psi/process/scrape_psi_reports.py)

A Playwright-based automation tool for downloading quality inspection reports from the PSI (Construction Hive) system. Downloads individual PDF reports with metadata tracking.

**Features:**
- Automated login with credentials from `.env`
- Pagination handling (10 documents per page)
- Individual PDF downloads with metadata extraction
- Idempotent operation via `manifest.json` tracking by document UUID
- Resume capability via `--start-offset` parameter
- `--force` flag to re-download existing documents
- `--dry-run` mode to preview what would be downloaded
- `--headless` mode for background operation

**Output Structure:**
```
{WINDOWS_DATA_DIR}/raw/psi/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ DFR_0306103-9671-O1.pdf    # Individual report PDFs
â”‚   â”œâ”€â”€ DFR_0306103-9670-O1.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ manifest.json                   # Download tracking with metadata
â””â”€â”€ scraper.log                     # Execution log
```

**Usage:**
```bash
# Download all documents (6309 total)
python scripts/psi/process/scrape_psi_reports.py

# Download with limit
python scripts/psi/process/scrape_psi_reports.py --limit 100

# Resume from specific offset
python scripts/psi/process/scrape_psi_reports.py --start-offset 500

# Run in headless mode
python scripts/psi/process/scrape_psi_reports.py --headless

# Force re-download existing files
python scripts/psi/process/scrape_psi_reports.py --force

# Dry run (show what would be downloaded)
python scripts/psi/process/scrape_psi_reports.py --dry-run
```

**Environment Variables (`.env`):**
- `PSI_BASE_URL` - Base URL (default: https://www.constructionhive.com/)
- `PSI_USERNAME` - Login email
- `PSI_PASSWORD` - Login password

### QC Logs (CPMS Inspection Tracking)

**Location:** `{WINDOWS_DATA_DIR}/raw/qc_logs/`

Excel exports from CPMS (Construction Project Management System) tracking all inspection requests across contractors. This is a consolidated view of inspection activity that complements the individual inspection reports from RABA and PSI.

**Data Structure:**
```
qc_logs/
â”œâ”€â”€ DAILY INSPECTION REQUESTS/     # 141 daily snapshot files
â”‚   â”œâ”€â”€ QA_QC Inspections 12-23-25 Official.xlsx
â”‚   â””â”€â”€ ...
â””â”€â”€ MASTER LIST/                   # Cumulative logs by discipline
    â”œâ”€â”€ 12172025_USA T1 Project_Inspection and Test Log.xlsx
    â””â”€â”€ 06112024_USA T1 Project_Inspection and Test Log.xlsm
```

**Daily Inspection Requests (61K+ records):**
| Column | Description |
|--------|-------------|
| Date | Inspection date |
| Time | Inspection time |
| Number | Sequential ID |
| IR Number | Inspection Request ID (prefix indicates contractor: YT=Yates, AG=Austin Bridge, SECAI, ABR) |
| Status | Accepted, Failure, Open, VOID, Re-Inspection Required, etc. |
| Template | Inspection type (759 unique types) |
| System / Equip/ Location | Location description |
| Inspector | Inspector name |

**Master List (by discipline: ARCH, MECH, ELEC):**
Additional fields include: Author Company, Module, Reasons for failure, Week, Year, ITP

**Key Metrics:**
- Status distribution: ~88% Accepted, ~3% Failure, ~3% Open, ~3% VOID
- Date range: 2023-02 through 2025-12
- Contractors: Yates (YT), Austin Bridge (AG), SECAI, ABR

**Use Cases:**
- Inspection volume trends over time
- Failure rate analysis by template/discipline
- Contractor performance comparison
- Re-inspection tracking and rework quantification
