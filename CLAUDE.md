# Samsung Taylor FAB1 Performance Analysis

## Project Context

**Client:** Samsung (via SECAI - Samsung Engineering and Construction America Inc.)
**Analyst:** MXI (Construction Claims Consultants)
**Project:** Taylor FAB1 Semiconductor Fabrication Facility, Taylor, Texas

### Entities & Roles

| Entity | Role |
|--------|------|
| **Samsung** | Owner - semiconductor fab facility |
| **SECAI** | Samsung's US construction arm, MXI's direct client |
| **Yates Construction** | General Contractor (GC) |
| **Subcontractors** | ~50+ trade contractors (Berg Electric, Performance Contracting, etc.) |
| **MXI** | Claims consultant analyzing schedule delays and cost impacts |

### Our Service

MXI is analyzing the project to support SECAI in understanding:
1. **What caused schedule delays** - Which tasks, contractors, and issues drove critical path slippage
2. **Where quality issues occurred** - Location-based analysis of inspections, failures, and rework
3. **How labor was consumed** - Resource allocation patterns and productivity analysis
4. **Impact quantification** - Connecting delays to specific causes for claims support

---

## Approach

### Iterative Data Exploration

We follow an iterative cycle for each data source:

```
[Discover] → [Extract] → [Explore] → [Process] → [Integrate] → [Analyze]
     ↑                                                              |
     └──────────────────────────────────────────────────────────────┘
```

1. **Discover** - Identify available data sources (PDFs, Excel, APIs, web portals)
2. **Extract** - Download/scrape raw data, preserve original format
3. **Explore** - Understand structure, identify patterns, assess quality
4. **Process** - Parse, normalize, enrich with dimension IDs
5. **Integrate** - Link across sources via shared dimensions
6. **Analyze** - Answer business questions, iterate as new questions arise

### Tools & Techniques

| Tool | Use Case |
|------|----------|
| **Regex** | Pattern extraction (room codes, grid coordinates, dates) |
| **LLM Parsing** | Unstructured PDF extraction (Gemini for RABA/PSI quality reports) |
| **Embeddings** | Semantic search across narratives and documents (ChromaDB + Gemini) |
| **Pandas** | Data transformation, aggregation, joining |
| **Playwright** | Web scraping (RABA, PSI, ProjectSight portals) |
| **Pydantic** | Schema validation for output files |

### Processing Philosophy

- **Idempotent scripts** - Running twice produces same result, safe to re-run
- **Incremental where possible** - Skip already-processed files
- **Schema validation** - Validate outputs before overwriting (Power BI stability)
- **Traceable** - Raw files preserved, processing is reproducible

---

## Repository Organization

### Folder Structure

```
samsung-project/
├── CLAUDE.md                    # This file - high-level overview
├── scripts/
│   ├── {source}/                # One folder per data source
│   │   ├── CLAUDE.md            # Source-specific documentation
│   │   └── process/             # Processing scripts
│   ├── shared/                  # Cross-source utilities
│   │   ├── dimension_lookup.py  # Get dimension IDs
│   │   ├── daily_refresh.py     # Update all pipelines
│   │   └── ...
│   └── integrated_analysis/     # Cross-source integration
│       ├── location/            # Centralized location processing
│       ├── dimensions/          # Dimension table builders
│       └── ...
├── schemas/                     # Pydantic schemas for output validation
├── src/
│   ├── config/settings.py       # Path configuration
│   └── document_processor/      # LLM-based PDF extraction pipeline
└── tests/
```

### Self-Documenting Sources

Each `scripts/{source}/` folder contains:
- `CLAUDE.md` - Purpose, data flow, usage examples (≤150 lines)
- `process/` - Processing scripts
- `process/run.sh` - Pipeline orchestrator

**Current sources:** `primavera/`, `tbm/`, `projectsight/`, `raba/`, `psi/`, `narratives/`

### Data Directory (Separate from Repo)

Data lives in a local directory specified by `WINDOWS_DATA_DIR` in `.env`:

```
{WINDOWS_DATA_DIR}/
├── raw/{source}/           # Source files as received (100% traceable)
│   ├── primavera/          # XER schedule files
│   ├── tbm/                # Daily plan Excel files
│   ├── projectsight/       # Daily reports JSON, NCR exports
│   ├── raba/               # Quality inspection PDFs
│   ├── psi/                # Quality inspection PDFs
│   └── ...
└── processed/{source}/     # Output files generated by this repo
    ├── tbm/work_entries_enriched.csv
    ├── raba/raba_psi_consolidated.csv
    ├── projectsight/labor_entries.csv
    └── integrated_analysis/dim_*.csv, map_*.csv
```

**Rules:**
- `raw/` is read-only, never modified by scripts
- `processed/` is generated output, can be regenerated from raw
- Git tracks code, not data (data is on OneDrive)

---

## Dimension Tables

Dimension tables enable joining across data sources. Located in `processed/integrated_analysis/` (flattened structure).

### dim_location (Most Complex)

The location dimension bridges data sources through a **grid-based spatial model**.

#### The Problem

Different sources describe locations differently:
- P6 Schedule: Room codes (`FAB116406`)
- Quality Reports: Grid coordinates (`G/10-12`)
- TBM Daily Plans: Building + Level + Area (`FAB, 2F, G~K/8~15`)
- Labor Data: No location at all

#### The Solution: Grid Bounds

Every location has grid bounds (row min/max, col min/max):

```
Grid System (same across all buildings):
     1    5    10   15   20   25   30
  A  ┌────┬────┬────┬────┬────┬────┐
  B  │    │    │    │    │    │    │
  C  │    │ ┌──┴────┴──┐ │    │    │   ← Room FAB116406
  D  │    │ │          │ │    │    │     grid: C-E / 8-12
  E  │    │ └──┬────┬──┘ │    │    │
  F  │    │    │    │    │    │    │
     └────┴────┴────┴────┴────┴────┘
```

**Key insight:** Grid coordinates are consistent across buildings (FAB, SUE, SUW, FIZ) and levels. A quality inspection at "G/10" on Level 2 can be matched to rooms on that level whose grid bounds contain G/10.

#### Location Hierarchy

| Type | Example | Grid Coverage |
|------|---------|---------------|
| ROOM | FAB116406 | Specific bounds (C-E/8-12) |
| STAIR | STR-21 | Point or small area |
| ELEVATOR | ELV-01 | Point (single grid cell) |
| GRIDLINE | G/10 | Point or range |
| LEVEL | FAB-2F | Entire level |
| BUILDING | FAB | Entire building |

#### Centralized Location Processing

**CRITICAL:** All location enrichment MUST use the centralized module:

```python
from scripts.integrated_analysis.location import enrich_location

result = enrich_location(
    building='FAB',
    level='2F',
    grid='G/10-12',
    source='RABA'
)

# Returns:
result.dim_location_id      # FK to dim_location
result.location_type        # ROOM, GRIDLINE, LEVEL, etc.
result.affected_rooms       # JSON array of rooms in grid bounds
result.affected_rooms_count # Number of rooms matched
```

See `scripts/integrated_analysis/location/CLAUDE.md` for full documentation.

### dim_company

Normalizes company names across sources (each source uses different spellings/abbreviations).

| Field | Description |
|-------|-------------|
| company_id | Primary key |
| canonical_name | Standardized name |
| primary_csi_section_id | FK to dim_csi_section |

**Aliases:** `map_company_aliases.csv` maps variants → canonical name
- "Berg Electric" → "Berg"
- "BERG ELEC" → "Berg"
- "Berg Electrical" → "Berg"

### dim_csi_section

CSI (Construction Specifications Institute) codes classify work types:

| Field | Description |
|-------|-------------|
| csi_section_id | Primary key |
| csi_code | Code like "03 30 00" |
| csi_title | "Cast-in-Place Concrete" |

CSI is inferred from:
1. Activity descriptions (keywords like "drywall", "concrete")
2. Company's primary trade
3. Division codes from source data

---

## Data Sources

### Power BI Fact Tables

| Source | File | Records | Location | Company | CSI |
|--------|------|---------|----------|---------|-----|
| TBM | `tbm/work_entries_enriched.csv` | 76K | 22% | 96% | 81% |
| RABA+PSI | `raba/raba_psi_consolidated.csv` | 15.7K | 100% | 94% | 99% |
| ProjectSight | `projectsight/labor_entries.csv` | 863K | 0%* | 100% | 100% |
| NCR | `projectsight/ncr_consolidated.csv` | 2K | N/A | 66% | 91% |

*ProjectSight labor data has no location in source

### Source Descriptions

| Source | Purpose | Raw Location |
|--------|---------|--------------|
| **Primavera P6** | Schedule snapshots (66 XER files) | `raw/primavera/` |
| **TBM** | Daily work plans from toolbox meetings | `raw/tbm/` |
| **ProjectSight** | Labor hours from daily reports | `raw/projectsight/` |
| **RABA** | Quality inspections (RKCI system) | `raw/raba/` |
| **PSI** | Quality inspections (Const Hive) | `raw/psi/` |
| **NCR** | Non-conformance records | `raw/projectsight/` |
| **Narratives** | Schedule narratives, weekly reports | `raw/narratives/` |

### Embeddings-Only Sources

Some sources are consumed via semantic search rather than structured tables:
- **Weekly Reports** - Narratives and issues searchable via embeddings
- **Schedule Narratives** - P6 narrative exports

```bash
python -m scripts.narratives.embeddings search "HVAC delays"
```

---

## Daily Operations

### Refresh All Data

Single command to update all Power BI data sources:

```bash
python -m scripts.shared.daily_refresh
python -m scripts.shared.daily_refresh --dry-run      # Preview only
python -m scripts.shared.daily_refresh --skip-scrapers # Fast local refresh
```

This runs (in order):
1. **Parsers** - TBM, P6, ProjectSight (incremental)
2. **Scrapers** - RABA, PSI (manifest-tracked)
3. **Dimensions** - Build dim_location
4. **Consolidation** - Enrich all fact tables with dimension IDs

### Environment Setup

Create `.env` in project root:

```bash
WINDOWS_DATA_DIR=/path/to/data/directory

# Scraper credentials
RABA_USERNAME=...
RABA_PASSWORD=...
PSI_BASE_URL=...
PSI_USERNAME=...
PSI_PASSWORD=...
PROJECTSIGHT_USERNAME=...
PROJECTSIGHT_PASSWORD=...
```

### Schema Validation

Output files are validated against Pydantic schemas before writing:

```python
from schemas.validator import validated_df_to_csv

validated_df_to_csv(df, output_path)  # Raises if schema mismatch
```

**Critical rule:** Never remove or rename columns in `processed/` files - Power BI depends on them.

Run schema tests:
```bash
pytest tests/unit/test_schemas.py -v
```

---

## Power BI Schema Integration

Power BI uses a JSON schema file to dynamically load CSV tables with correct column types. This schema is **derived from Pydantic definitions** (not inferred from CSV data) to ensure only approved schemas are used.

### How It Works

```
┌─────────────────────┐     ┌──────────────────────┐     ┌─────────────────────┐
│  Pydantic Schemas   │ ──► │  powerbi_schema.json │ ──► │  Power Query (M)    │
│  (schemas/*.py)     │     │  (processed/)        │     │  AllTables query    │
└─────────────────────┘     └──────────────────────┘     └─────────────────────┘
        │                            │
        │  Defines columns           │  Provides paths
        │  and types                 │  and typed columns
        ▼                            ▼
┌─────────────────────┐     ┌──────────────────────┐
│  Pipeline outputs   │ ◄── │  Validation: CSV     │
│  (processed/*.csv)  │     │  must match Pydantic │
└─────────────────────┘     └──────────────────────┘
```

### Daily Refresh Integration

Schema generation is the **final phase** of the daily refresh pipeline:

```
PREFLIGHT → PARSE → SCRAPE → CONSOLIDATE → VALIDATE → COMMIT → POWERBI SCHEMA
```

If a CSV doesn't match its Pydantic schema, the pipeline **fails** with an error.

### Schema Coverage

Only tables with registered Pydantic schemas are included in `powerbi_schema.json`:

```bash
# Check which tables have Pydantic schemas
python -m scripts.shared.generate_powerbi_schema --list
```

Current coverage: **25 tables** with full Pydantic schema coverage including:
- Dimension tables (`dim_location`, `dim_company`, `dim_csi_section`)
- Fact tables (`work_entries`, `labor_entries`, `ncr_consolidated`, `raba_psi_consolidated`)
- Quality tables (`qc_inspections_enriched`, `combined_qc_inspections`)
- Fieldwire tables (`fieldwire_combined`, `fieldwire_checklists`)
- Data quality tables (all `*_data_quality.csv` files)

### Adding a New Table to Power BI

1. **Create Pydantic schema** in `schemas/`:
   ```python
   # schemas/fieldwire.py
   from pydantic import BaseModel
   from typing import Optional

   class FieldwireCombined(BaseModel):
       id: str
       title: str
       status: Optional[str] = None
       # ... all columns
   ```

2. **Register in `schemas/registry.py`**:
   ```python
   from .fieldwire import FieldwireCombined

   SCHEMA_REGISTRY = {
       # ... existing entries
       'fieldwire_combined.csv': FieldwireCombined,
   }
   ```

3. **Run daily refresh** (or just schema generation):
   ```bash
   python -m scripts.shared.generate_powerbi_schema
   ```

4. **Refresh Power BI** - The new table will appear with correct types

### Failure Scenarios

| Scenario | Result |
|----------|--------|
| CSV matches Pydantic schema | ✓ Included in Power BI schema |
| CSV has extra columns | ✓ Included (extras not typed) |
| CSV missing Pydantic columns | ✗ **Pipeline fails** |
| No Pydantic schema registered | ⚠ Skipped with warning |

### Power Query Usage

In Power BI, create a query called `AllTables` that loads all registered tables with correct types:

```powerquery
let
    JsonPath = DataPath & "\processed\powerbi_schema.json",
    JsonContent = Json.Document(File.Contents(JsonPath)),
    Tables = JsonContent[tables],
    #"Converted to Table" = Record.ToTable(Tables),

    // Load CSV content for each table
    #"Added Content" = Table.AddColumn(#"Converted to Table", "Content", each
        let
            filepath = DataPath & "\" & Record.Field([Value], "path"),
            content = File.Contents(filepath)
        in content
    ),

    // Parse CSV
    #"Added Csv" = Table.AddColumn(#"Added Content", "Csv", each
        Csv.Document([Content], [Delimiter = ",", Encoding = 65001, QuoteStyle = QuoteStyle.Csv])
    ),

    // Promote headers
    #"Added Headers" = Table.AddColumn(#"Added Csv", "WithHeaders", each
        Table.PromoteHeaders([Csv], [PromoteAllScalars = true])
    ),

    // Type mapping
    TypeMap = [
        #"Text.Type" = Text.Type,
        #"Int64.Type" = Int64.Type,
        #"Number.Type" = Number.Type,
        #"Logical.Type" = Logical.Type,
        #"DateTime.Type" = DateTime.Type,
        #"Date.Type" = Date.Type
    ],

    // Apply types from schema
    #"Applied Types" = Table.AddColumn(#"Added Headers", "Data", each
        let
            Columns = Record.Field([Value], "columns"),
            ColumnTypes = List.Transform(Columns,
                each {Record.Field(_, "name"), Record.Field(TypeMap, Record.Field(_, "type"))}
            ),
            TypedTable = Table.TransformColumnTypes([WithHeaders], ColumnTypes)
        in TypedTable
    ),

    // Keep only Name and Data columns
    #"Final" = Table.SelectColumns(#"Applied Types", {"Name", "Data"})
in
    #"Final"
```

This produces a table with `Name` (e.g., "tbm/work_entries") and `Data` (the typed table).

**Note:** A standalone function approach triggers Power BI's Formula.Firewall error due to mixing query references with data source access. The `Table.AddColumn` pattern avoids this by keeping all data source access within a single query context.

---

## Navigation

| Topic | Location |
|-------|----------|
| Location processing | `scripts/integrated_analysis/location/CLAUDE.md` |
| TBM pipeline | `scripts/tbm/CLAUDE.md` |
| ProjectSight pipeline | `scripts/projectsight/CLAUDE.md` |
| RABA pipeline | `scripts/raba/CLAUDE.md` |
| PSI pipeline | `scripts/psi/CLAUDE.md` |
| Embeddings search | `scripts/narratives/CLAUDE.md` |
| Document processor | `src/document_processor/CLAUDE.md` |
| Schedule slippage | `scripts/integrated_analysis/CLAUDE.md` |
| Shared utilities | `scripts/shared/CLAUDE.md` |

### Key Scripts

| Script | Purpose |
|--------|---------|
| `scripts/shared/daily_refresh.py` | Update all pipelines |
| `scripts/shared/generate_powerbi_schema.py` | Generate Power BI schema from Pydantic |
| `scripts/shared/dimension_lookup.py` | Get dimension IDs |
| `scripts/integrated_analysis/location/` | Centralized location enrichment |
| `scripts/integrated_analysis/dimensions/build_dim_location.py` | Build location dimension |
| `scripts/integrated_analysis/data_quality/dimension_coverage/` | **Dimension coverage report** |

### Data Quality Report

Check dimension coverage across all sources:

```bash
python -m scripts.integrated_analysis.data_quality.dimension_coverage
```

This generates a comprehensive report showing:
- Coverage matrix (Source × Dimension)
- Location granularity breakdown (ROOM vs GRIDLINE vs BUILDING)
- CSI section joinability between sources
- Unresolved company names
- Actionable recommendations with file paths
